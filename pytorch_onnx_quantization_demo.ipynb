{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP54tasF3APV2XPHGUBbLrQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shra1surya/embedded-ai-model-export/blob/main/pytorch_onnx_quantization_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTN5l96QkPZI",
        "outputId": "d89b4fc1-6b07-4422-b552-04d9eda91d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.1)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, onnxruntime, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 onnx-1.18.0 onnxruntime-1.22.1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx onnxruntime torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "import os\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57w_6ESdlfDQ",
        "outputId": "024d5385-8fa5-49d6-bf7f-f71ef37ec8fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small CNN with two convolutional layers, two fully connected layers. Perfect for testing ONNX and quantization. Runs fast and fits in embedded constraints."
      ],
      "metadata": {
        "id": "C5Dwgy-rl8MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1,8,3,1)\n",
        "    self.conv2 = nn.Conv2d(8,16,3,1)\n",
        "    self.fc1 = nn.Linear(5*5*16, 50)\n",
        "    self.fc2 = nn.Linear(50,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x,2)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x,2)\n",
        "    x = x.view(-1, 5*5*16)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sYn4lZdYl0nA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 3: Load Pretrained Model or Train It (Quick)\n",
        "\n",
        "You can skip training and load a pretrained version — but for completeness, do this:"
      ],
      "metadata": {
        "id": "ztl5Jo5UoG2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "model = SimpleCNN()\n",
        "optimizer =torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = loss_fn(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f\"Epoch {epoch+1} completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieAcxVomoOxv",
        "outputId": "5e88345b-eaae-4dd6-9bd0-5eea315c6bfb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 483kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.53MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed.\n",
            "Epoch 2 completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step4: Export to ONNX (Open Neural Network Exchange)"
      ],
      "metadata": {
        "id": "HW1vTQKxud3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch model to evaluation mode before export\n",
        "model.eval()\n",
        "\n",
        "# Create a dummy input — same shape as one MNIST image: (batch_size=1, channels=1, height=28, width=28)\n",
        "dummy_input = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "\n",
        "# Export the model to ONNX\n",
        "torch.onnx.export(\n",
        "    model,                      # Your trained model\n",
        "    dummy_input,                # Input tensor\n",
        "    \"mnist_model.onnx\",         # Output ONNX file name\n",
        "    export_params=True,         # Store trained weights\n",
        "    opset_version=11,           # ONNX version\n",
        "    do_constant_folding=True,   # Optimize constants\n",
        "    input_names=['input'],      # Optional naming\n",
        "    output_names=['output']\n",
        ")\n",
        "\n",
        "print(\"Exported to mnist_model.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAFsijQmuiX0",
        "outputId": "1bccf666-d23d-4fc1-a6ec-20d4b7283f64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Exported to mnist_model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting ONNX → It lets you take a model trained in PyTorch (or TensorFlow) and run it anywhere — including embedded runtimes (ONNX Runtime, TVM, Edge Impulse, etc.)"
      ],
      "metadata": {
        "id": "QVccgrcmvGcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Load and Run Inference with ONNX Runtime"
      ],
      "metadata": {
        "id": "DFhCvpXvvdhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(\"mnist_model.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\" ONNX model is valid\")\n",
        "\n",
        "# Create ONNX Runtime session\n",
        "ort_session = ort.InferenceSession(\"mnist_model.onnx\")\n",
        "\n",
        "# Use a sample MNIST image for inference\n",
        "sample_input = torch.randn(1, 1, 28, 28)\n",
        "input_numpy = sample_input.numpy()\n",
        "\n",
        "# Run inference\n",
        "outputs = ort_session.run(None, {\"input\": input_numpy})\n",
        "predicted_digit = np.argmax(outputs[0])\n",
        "\n",
        "print(f\"Predicted Digit: {predicted_digit}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE-Yxr2svkLs",
        "outputId": "57c4eb09-c6c3-40cf-b0b9-a982d6cd2947"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ONNX model is valid\n",
            "Predicted Digit: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative: Load a real MNIST image and visualization"
      ],
      "metadata": {
        "id": "jNXtUzDJ0Z_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#load one real sample from the MNIST test set\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('./data', train = False, download = True, transform = transform),\n",
        "    batch_size = 1,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Get one sample and its label\n",
        "data_iter = iter(test_loader)\n",
        "real_sample, real_label = next(data_iter)\n",
        "\n",
        "# Plot the image\n",
        "plt.imshow(real_sample[0][0], cmap=\"gray\")\n",
        "plt.title(f\"Actual Digit: {real_label.item()}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Prepare the sample for ONNX Inference\n",
        "real_input_numpy = real_sample.numpy()\n",
        "onnx_output = ort_session.run(None, {\"input\": real_input_numpy})\n",
        "predicted_digit = np.argmax(onnx_output[0])\n",
        "\n",
        "print(f\"ONNX Prediction: {predicted_digit} | Actual Label: {real_label.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "jW5f-bze0kQ-",
        "outputId": "338024f8-2b67-4773-ceb0-18edb960210b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAErVJREFUeJzt3H2sV3UdwPHPDy5wDRV5dFP0ApcVOAYVKFEhyJYPM50pD5EG16ZWm8bYombzAW3rwfAhyWIZiV3upA1wMLTMBWyWViCDhosmdAFJMpEnCQqB0x+OzyQQ77ney8V4vTb+4Pc7n9/5/u70977nnB+nUhRFEQAQEe3aegEAnDxEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEgf97lUolpk+f3uKvu3z58qhUKrF8+fLSsxs3boxKpRJz5sxp8XXB+yEKlPLjH/84KpVKDB8+vNmv8eqrr8b06dNj9erVLbew9+nwh/ThPx06dIgePXrEJz/5yfjWt74VmzdvbvU1PP300y0erw0bNkR1dXVUKpVYuXJli742/59EgVIaGhqiT58+8ac//SnWr1/frNd49dVX45577jmponDYxIkTo76+PmbPnh133nln9OvXLx566KEYOHBgzJs374htL7744ti3b19cfPHFpfdTU1MT+/btiy9+8Yv52NNPPx333HPP+34P7zR16tSoqqpq0dfk/5so0GSNjY3x/PPPxwMPPBA9e/aMhoaGtl5Si/v4xz8eN9xwQ0yaNCmmTJkSc+fOjb/85S/Ru3fvmDx5cqxZsya3bdeuXVRXV0e7duX/N6pUKlFdXR3t27dvyeUf4Zlnnolnnnkmpk6d2mr74P+PKNBkDQ0N0bVr17jyyitj7Nix7xqFnTt3xtSpU6NPnz7RqVOn6N27d0yaNCm2bdsWy5cvjwsvvDAiIm688cY8XXP43HqfPn2irq7uqNccPXp0jB49Ov++f//+uOuuu2Lo0KHRpUuX6Ny5c4wcOTKWLVvW0m87ampqYs6cObF///6477778vF3u6bwyCOPRL9+/eK0006Liy66KJ577rmj1v+/1xTq6urikUceiYg44jTWYVu3bo1169bFW2+91aQ1v/XWWzFlypSYMmVK1NbWNu+Nc0oSBZqsoaEhrr322ujYsWNMnDgxXn755VixYsUR2+zZsydGjhwZM2fOjEsvvTR++MMfxle+8pVYt25dbNmyJQYOHBj33ntvRETccsstUV9fH/X19aVPwezevTt+9rOfxejRo+P73/9+TJ8+PV5//fW47LLLWuW01IgRI6K2tjaeffbZ4273k5/8JG699dbo3bt33HfffTFy5Mi45pprYsuWLced+/KXvxyf+cxnIiLyZ1JfX5/P33777TFw4MD4+9//3qT1PvTQQ7Fjx4644447mrQ9HOZkI03y4osvxrp162LmzJkREfHpT386evfuHQ0NDfmbf0TED37wg1i7dm0sXLgwPve5z+Xjd9xxRxRFEZVKJa644oq46667YsSIEXHDDTc0az1du3aNjRs3RseOHfOxm2++OQYMGBAzZ86M2bNnN/OdvrtBgwbFokWLYvfu3XHmmWce9fz+/fvjzjvvjAsvvDCWLl2a5/IHDx4cdXV10bt373d97REjRsSHP/zhePbZZ5v9MznsH//4R3z729+OGTNmHHOdcDyOFGiShoaGOPvss+OSSy6JiLdPcUyYMCHmzZsXBw8ezO0WLFgQQ4YMOSIIh73zdMj71b59+wzCoUOHYvv27XHgwIEYNmxYrFq1qsX2806nn356RES8+eabx3x+5cqV8cYbb8TNN998xMXd66+/Prp27fq+9j1nzpwoiiL69Onzntt+85vfjH79+sVNN930vvbJqUkUeE8HDx6MefPmxSWXXBKNjY2xfv36WL9+fQwfPjxee+21+O1vf5vbbtiwIQYNGnRC1vX444/H4MGDo7q6Orp37x49e/aMp556Knbt2tUq+9uzZ09ERJxxxhnHfH7Tpk0REdG/f/8jHq+qqmrSh3lL+MMf/hD19fXx4IMPNusCODh9xHtaunRpbN26NebNm3fU1zIj3j6KuPTSS1tkX+92NHHw4MEjvqkzd+7cqKuri2uuuSamTZsWvXr1ivbt28d3v/vd2LBhQ4us5X+tXbs2evXqdVKfkvnGN74RI0eOjL59+8bGjRsjImLbtm0R8fbF6s2bN8f555/fhivkZCcKvKeGhobo1atXfjvmnRYuXBhPPvlkzJo1K0477bSora2NtWvXHvf1jncaqWvXrrFz586jHt+0aVP069cv/z5//vzo169fLFy48IjXu/vuu5vwjsp74YUXYsOGDcc9319TUxMREevXr8/TbBERBw4ciI0bN8bgwYOPu4+WOL22efPm2LRpU/Tt2/eo566++uro0qXLMX++cJgocFz79u2LhQsXxrhx42Ls2LFHPX/OOefEE088EYsXL44JEybEddddF/fee288+eSTR11XOHyhuXPnzhERx/xwqq2tjeeeey7279+f1wyWLFkSr7zyyhFROHzUcPg1IyL++Mc/xgsvvNDivwlv2rQp6urqomPHjjFt2rR33W7YsGHRvXv3ePTRR+PGG2/M6woNDQ2xY8eO99zPO38uZ5111hHPbd26NXbt2hW1tbXRoUOHd32Nn/70p7F3794jHlu6dGnMnDkzZsyYEQMGDHjPdXBqEwWOa/HixfHmm2/G1VdffcznP/GJT+Q/ZJswYUJMmzYt5s+fH+PGjYsvfelLMXTo0Ni+fXssXrw4Zs2aFUOGDIna2to466yzYtasWXHGGWdE586dY/jw4dG3b9+46aabYv78+XH55ZfH+PHjY8OGDTF37tyjvmv/2c9+Nr/hdOWVV0ZjY2PMmjUrLrjggjz33xyrVq2KuXPnxqFDh2Lnzp2xYsWKWLBgQVQqlaivrz/ub/sdO3aM6dOnx2233RZjxoyJ8ePHx8aNG2POnDlRW1v7nkcCQ4cOjYiIr33ta3HZZZdF+/bt4/Of/3xEvP2V1McffzwaGxuPe33iWKfxDsd31KhRMWzYsPf4CXDKK+A4rrrqqqK6urr417/+9a7b1NXVFR06dCi2bdtWFEVRvPHGG8Wtt95anHvuuUXHjh2L3r17F5MnT87ni6IoFi1aVFxwwQVFVVVVERHFY489ls/df//9xbnnnlt06tSp+NSnPlWsXLmyGDVqVDFq1Kjc5tChQ8V3vvOdoqampujUqVPxsY99rFiyZEkxefLkoqam5oj1RURx9913H/d9NjY2FhGRf6qqqopu3boVw4cPL26//fZi06ZNR80sW7asiIhi2bJlRzz+8MMP57ouuuii4ve//30xdOjQ4vLLLz9qf+983wcOHChuu+22omfPnkWlUine+b/n5MmTi4goGhsbj/s+juWxxx4rIqJYsWJF6VlOPZWiKIq2ChKcCg4dOhQ9e/aMa6+9Nh599NG2Xg4cl++sQQv697//Hf/7e9YvfvGL2L59+xG3uYCTlSMFaEHLly+PqVOnxrhx46J79+6xatWqmD17dgwcODBefPHFI/4FNpyMXGiGFtSnT58477zz4uGHH47t27dHt27dYtKkSfG9731PEPhAcKQAQHJNAYAkCgCkJl9TaMk7XAJw4jXlaoEjBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBSVVsvgGObOHFis+ZmzJhReubrX/966Zknnnii9AzvT5cuXUrPjBkzpvTM3/72t9Iza9asKT3DycmRAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkhvinaRGjx7drLlzzjmn9MyQIUNKz7gh3olXX19feuajH/1o6ZmlS5eWnqmrqys9w8nJkQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIb4p0AHTp0KD3zhS98oRVWcmzt2vnd4IPg/PPPLz1z3nnnlZ7ZtWtX6ZlKpVJ6piiK0jO0Pp8GACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAqhRNvFVhc+6CyNs6duxYeuY///lPK6zk2Pbu3Vt6pnPnzq2wEo5n9erVpWeGDBnS8gs5hkGDBpWeeemll1phJRxPUz7uHSkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACBVtfUCTgWHDh0qPbN27dpm7as5Nyarrq4uPXPFFVeUnvnVr35VeoYPhtra2tIzboh3cnKkAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5IZ4J0C7duXbO2DAgFZYybE1Z309evRohZXwQXX22We39RJoIY4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQ3BDvBNi/f3/pmZ///OfN2tctt9xSeubAgQOlZ/bs2VN6hg+GLVu2lJ5ZsmRJK6yEtuBIAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASO6SepKaP39+s+aac5fUqqry/xk88MADpWd+97vflZ6JiHj99debNUfz/POf/yw9s2vXrlZYCW3BkQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIb4p2kXnnllWbNHThwoPRMc26I16dPn9IzL7/8cumZiIj+/fuXntm2bVuz9lVWu3blf68aM2ZMs/Y1aNCgZs2V1Zyb2+3du7cVVkJbcKQAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDkhngnqXXr1jVrbsGCBaVnJkyY0Kx9ldWlS5dmzT344IOlZ370ox+VnhkwYEDpmauuuqr0zHXXXVd6Bk4URwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiVoiiKJm1YqbT2WmgB3bp1Kz3z17/+tfRMjx49Ss/wwfDVr3619MysWbNaYSW0tKZ83DtSACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAqmrrBdCytm/fXnrm+uuvLz2zaNGi0jPV1dWlZzjxXnvttbZeAm3IkQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJDcJZX4zW9+U3pm7NixpWfuv//+0jMRER/5yEeaNXci7N69u/TMSy+91Kx9jRgxollzUIYjBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJDfEo1meeuqp0jPPP/98s/ZVU1NTembIkCGlZ9asWVN65swzzyw9s2jRotIzcKI4UgAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQHJDPE6YHTt2nLC51atXN2tfZfXo0aP0TKdOnVphJS3n9NNPb+sl0IYcKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIFWKoiiatGGl0tprgVPCL3/5y2bNjR8/voVXcmzLli0rPTNmzJhWWAktrSkf944UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQqtp6AcDJZfjw4aVn+vfvX3pm/fr1pWdofY4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5C6pwBE+9KEPlZ7p1KlTK6yEtuBIAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyQ3xgCP8+c9/Lj2zefPmVlgJbcGRAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUqUoiqJJG1Yqrb0WOCUMHTq0WXO//vWvS88cPHiw9MywYcNKz2zZsqX0DCdeUz7uHSkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACC5IR7AKcIN8QAoRRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkqqZuWBRFa64DgJOAIwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUA0n8BS9w0MXWu6fAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Prediction: 4 | Actual Label: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "output:\n",
        "![real_digit.png](data:image/png;base64,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)\n",
        "ONNX Prediction: 4 | Actual Label: 4"
      ],
      "metadata": {
        "id": "oPS4sZao2lFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Quantize the ONNX Model"
      ],
      "metadata": {
        "id": "7oYmpr-Oxbyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "quantized_model_path = \"mnist_model_quant.onnx\"\n",
        "quantize_dynamic(\n",
        "    model_input = \"mnist_model.onnx\",\n",
        "    model_output=quantized_model_path,\n",
        "    weight_type=QuantType.QInt8\n",
        ")\n",
        "\n",
        "print(f\"QUantized model save to {quantized_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loV02xCoxiSv",
        "outputId": "95206647-981a-4207-f6b0-6c7399566cd3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUantized model save to mnist_model_quant.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the file size of mnist_model.onnx and mnist_model_quant.onnx\n",
        "```\n",
        "/content# ls -l\n",
        "total 124\n",
        "drwxr-xr-x 3 root root  4096 Aug  8 12:29 data\n",
        "-rw-r--r-- 1 root root 88756 Aug  8 12:52 mnist_model.onnx\n",
        "-rw-r--r-- 1 root root 27759 Aug  8 13:06 mnist_model_quant.onnx\n",
        "drwxr-xr-x 1 root root  4096 Aug  6 13:39 sample_data\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PDKTXs2EzN6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Compare Model Sizes (real and dummy data)"
      ],
      "metadata": {
        "id": "d5hFF3ZV4H9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh mnist_model*.onnx # with real data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MylcKhjq3sVv",
        "outputId": "db8eaeee-2181-487d-81ff-f737d015ef99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 87K Aug  8 12:52 mnist_model.onnx\n",
            "-rw-r--r-- 1 root root 28K Aug  8 13:06 mnist_model_quant.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Benchmark Inference Time"
      ],
      "metadata": {
        "id": "VI-bIYMr4B7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Run inference multiple times for averaging\n",
        "def benchmark_model(session, input_data, runs=100):\n",
        "  start = time.time()\n",
        "  for _ in range(runs):\n",
        "    _ = session.run(None, {\"input\": input_data})\n",
        "  end = time.time()\n",
        "  avg_time_ms = (end - start) * 1000 / runs\n",
        "  return avg_time_ms\n",
        "\n",
        "# Run both original and quantized models\n",
        "ort_session_orig = ort.InferenceSession(\"mnist_model.onnx\")\n",
        "ort_session_quant = ort.InferenceSession(\"mnist_model_quant.onnx\")\n",
        "\n",
        "real_input_numpy = real_sample.numpy()\n",
        "\n",
        "avg_time_orig = benchmark_model(ort_session_orig, real_input_numpy)\n",
        "avg_time_quant = benchmark_model(ort_session_quant, real_input_numpy)\n",
        "\n",
        "\n",
        "print(f\"Original model inference time: {avg_time_orig:.3f} ms\")\n",
        "print(f\"Quantized model inference time: {avg_time_quant:.3f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "9g1SIllF4UJc",
        "outputId": "03445f37-4959-41a3-e627-f8220b52b44f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplemented",
          "evalue": "[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/conv1/Conv_quant'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplemented\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4146577731.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Run both original and quantized models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mort_session_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist_model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mort_session_quant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist_model_quant.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mreal_input_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_inference_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisabled_optimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m_create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# initialize the C++ InferenceSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisabled_optimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplemented\u001b[0m: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/conv1/Conv_quant'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acbd0484"
      },
      "source": [
        "Let's try a different quantization method that might be compatible with the current ONNX Runtime version. We can try dynamic quantization with `QuantType.QUInt8` which might be more widely supported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a14b4ed8",
        "outputId": "ca04c71d-9143-40f8-99a5-7bab487e6304"
      },
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "quantized_model_path_uint8 = \"mnist_model_quant_uint8.onnx\"\n",
        "quantize_dynamic(\n",
        "    model_input=\"mnist_model.onnx\",\n",
        "    model_output=quantized_model_path_uint8,\n",
        "    weight_type=QuantType.QUInt8\n",
        ")\n",
        "\n",
        "print(f\"Quantized model saved to {quantized_model_path_uint8}\")\n",
        "\n",
        "# Now, let's try to benchmark this new quantized model\n",
        "import time\n",
        "\n",
        "def benchmark_model(session, input_data, runs=100):\n",
        "  start = time.time()\n",
        "  for _ in range(runs):\n",
        "    _ = session.run(None, {\"input\": input_data})\n",
        "  end = time.time()\n",
        "  avg_time_ms = (end - start) * 1000 / runs\n",
        "  return avg_time_ms\n",
        "\n",
        "ort_session_orig = ort.InferenceSession(\"mnist_model.onnx\")\n",
        "ort_session_quant_uint8 = ort.InferenceSession(quantized_model_path_uint8)\n",
        "\n",
        "# Use a sample MNIST image for inference\n",
        "sample_input = torch.randn(1, 1, 28, 28)\n",
        "input_numpy = sample_input.numpy()\n",
        "\n",
        "avg_time_orig = benchmark_model(ort_session_orig, input_numpy)\n",
        "avg_time_quant_uint8 = benchmark_model(ort_session_quant_uint8, input_numpy)\n",
        "\n",
        "\n",
        "print(f\"Original model inference time: {avg_time_orig:.3f} ms\")\n",
        "print(f\"Quantized (UInt8) model inference time: {avg_time_quant_uint8:.3f} ms\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model saved to mnist_model_quant_uint8.onnx\n",
            "Original model inference time: 0.023 ms\n",
            "Quantized (UInt8) model inference time: 0.043 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p embedded_ai_project\n",
        "!mkdir -p embedded_ai_project/models\n",
        "!mv mnist_model*.onnx embedded_ai_project/models/\n",
        "!mv sample_data embedded_ai_project/ 2>/dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CkOMPIj-Fga",
        "outputId": "036059e9-7aa9-46d6-f9c4-30a1a96de80d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'mnist_model*.onnx': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('pytorch_onnx_quantization_demo.ipynb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "bYKgOwHU-krE",
        "outputId": "c1d355cf-d1d5-4045-a1bf-3d82f5917b71"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: pytorch_onnx_quantization_demo.ipynb",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3682365474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch_onnx_quantization_demo.ipynb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: pytorch_onnx_quantization_demo.ipynb"
          ]
        }
      ]
    }
  ]
}