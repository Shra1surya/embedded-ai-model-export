# embedded-ai-model-export
Hands-on project: PyTorch â†’ ONNX â†’ Quantization â†’ Inference (MNIST example)

# PyTorch â†’ ONNX â†’ Quantization Demo (MNIST)

This is a hands-on mini-project to demonstrate:
- Training a simple CNN using PyTorch on the MNIST dataset
- Exporting the model to ONNX format
- Quantizing the ONNX model to reduce its size
- Running inference using ONNX Runtime
- Benchmarking performance of original vs quantized models

---

## ðŸ§ª Results

### Prediction:
Expected inference for: (digit 4)

![Digit Prediction](./real_digit.png)


**Outcome:**
output: ONNX Prediction: 4 | Actual Label: 4

### Model Size Comparison:

-rw-r--r-- 1 root root 87K Aug  8 12:52 mnist_model.onnx
-rw-r--r-- 1 root root 28K Aug  8 13:06 mnist_model_quant.onnx

### Execution time comparison:

| Model                 | Size     | Inference Time (CPU) |
|----------------------|----------|-----------------------|
| `mnist_model.onnx`   | ~1.3 MB  | ~0.023 ms             |
| `mnist_model_quant_uint8.onnx` | ~450 KB | ~0.043 ms  |

> âš ï¸ ONNX Runtime on x86 doesnâ€™t accelerate ConvInteger ops. Better gains expected on ARM/embedded targets.

## ðŸ“¦ TFLite Results (Trained TF â†’ TFLite)

- **TFLite (optimized float)**: `mnist_model.tflite` â†’ **27.16 KB**
- **TFLite FP16**: `mnist_model_fp16.tflite` â†’ **46.78 KB**
- **Accuracy checks**:
  - Method A (Keras test sample): âœ… Pred **7**, Label **7**
  - Method B (torchvision sample): âœ… Pred **7**, Label **7**
- **Latency (CPU, 100 runs avg)**: **0.025 ms**

> Notes:
> - ONNX dynamic quantization reduced size on the ONNX path, but ONNX Runtime CPU didnâ€™t accelerate ConvInteger ops.
> - TFLite conversion succeeded; on this tiny network, the default optimized float model is smaller than FP16.
> - Real embedded targets (ARM/NPUs/MCUs) typically show bigger benefits from INT8 quantization and accelerator kernels.

---

## âœ… What I Learned

* [Day 1: PyTorch â†’ ONNX â†’ Quant](./day1_pytorch_to_onnx_quant.ipynb)
* [Day 2/3: TF â†’ TFLite + Benchmarks](./day2_day3_tflite_conversion_and_inference.ipynb)

- How to convert a PyTorch model to ONNX
- How to apply dynamic quantization with ONNX Runtime
- How quantization affects model size and inference time
- Practical insights into edge/embedded AI deployment pipelines

---

## ðŸ“‚ Project Structure

embedded-ai-model-export/
â”œâ”€â”€ day1_pytorch_to_onnx_quant.ipynb
â”œâ”€â”€ day2_day3_tflite_conversion_and_inference.ipynb
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mnist_model.onnx
â”‚   â”œâ”€â”€ mnist_model_quant_uint8.onnx
â”‚   â”œâ”€â”€ mnist_model.tflite
â”‚   â””â”€â”€ mnist_model_fp16.tflite
â”œâ”€â”€ README.md


---

## ðŸš€ Next Steps

- Try converting to TFLite and run on microcontrollers
- Test ONNX inference on Raspberry Pi or ARM boards
- Explore static quantization or QAT

---

## ðŸ§  Author

**Shravan Suryanarayana**  
System Software Architect | Embedded AI Explorer  
[LinkedIn](https://linkedin.com/in/shravansurya)
