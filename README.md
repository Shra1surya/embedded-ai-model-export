# embedded-ai-model-export
Hands-on project: PyTorch â†’ ONNX â†’ Quantization â†’ Inference (MNIST example)

[![Open In Colab - Day 1](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Shra1surya/embedded-ai-model-export/blob/main/day1_pytorch_to_onnx_quant.ipynb)
[![Open In Colab - Day 2/3](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Shra1surya/embedded-ai-model-export/blob/main/day2_day3_tflite_conversion_and_inference.ipynb)


# PyTorch â†’ ONNX â†’ Quantization Demo (MNIST)

This is a hands-on mini-project to demonstrate:
- Training a simple CNN using PyTorch on the MNIST dataset
- Exporting the model to ONNX format
- Quantizing the ONNX model to reduce its size
- Running inference using ONNX Runtime
- Benchmarking performance of original vs quantized models

---
## ðŸ§ª Reproduce locally (CPU)

```bash
# 1) Create a clean env (recommended)
python -m venv .venv && source .venv/bin/activate    # Windows: .venv\Scripts\activate

# 2) Install deps
pip install -r requirements.txt

# 3) Run Day 1 (PyTorch â†’ ONNX â†’ Quantization)
# Open the notebook and run all cells:
#   day1_pytorch_to_onnx_quant.ipynb

# 4) Run Day 2/3 (TF â†’ TFLite â†’ Inference + Benchmark)
# Open the notebook and run all cells:
#   day2_day3_tflite_conversion_and_inference.ipynb

## ðŸ§ª Results

### Prediction:
Expected inference for: (digit 4)

![Digit Prediction](./real_digit.png)


**Outcome:**
output: ONNX Prediction: 4 | Actual Label: 4

### Model Size Comparison:

-rw-r--r-- 1 root root 87K Aug  8 12:52 mnist_model.onnx
-rw-r--r-- 1 root root 28K Aug  8 13:06 mnist_model_quant.onnx

### Execution time comparison:

| Model                 | Size     | Inference Time (CPU) |
|----------------------|----------|-----------------------|
| `mnist_model.onnx`   | ~1.3 MB  | ~0.023 ms             |
| `mnist_model_quant_uint8.onnx` | ~450 KB | ~0.043 ms  |

> âš ï¸ ONNX Runtime on x86 doesnâ€™t accelerate ConvInteger ops. Better gains expected on ARM/embedded targets.

## ðŸ“¦ TFLite Results (Trained TF â†’ TFLite)

- **TFLite (optimized float)**: `mnist_model.tflite` â†’ **27.16 KB**
- **TFLite FP16**: `mnist_model_fp16.tflite` â†’ **46.78 KB**
- **Accuracy checks**:
  - Method A (Keras test sample): âœ… Pred **7**, Label **7**
  - Method B (torchvision sample): âœ… Pred **7**, Label **7**
- **Latency (CPU, 100 runs avg)**: **0.025 ms**

> Notes:
> - ONNX dynamic quantization reduced size on the ONNX path, but ONNX Runtime CPU didnâ€™t accelerate ConvInteger ops.
> - TFLite conversion succeeded; on this tiny network, the default optimized float model is smaller than FP16.
> - Real embedded targets (ARM/NPUs/MCUs) typically show bigger benefits from INT8 quantization and accelerator kernels.

---

## âœ… What I Learned

* [Day 1: PyTorch â†’ ONNX â†’ Quant](./day1_pytorch_to_onnx_quant.ipynb)
* [Day 2/3: TF â†’ TFLite + Benchmarks](./day2_day3_tflite_conversion_and_inference.ipynb)

- How to convert a PyTorch model to ONNX
- How to apply dynamic quantization with ONNX Runtime
- How quantization affects model size and inference time
- Practical insights into edge/embedded AI deployment pipelines

---

## ðŸ“‚ Project Structure

embedded-ai-model-export/
â”œâ”€â”€ day1_pytorch_to_onnx_quant.ipynb
â”œâ”€â”€ day2_day3_tflite_conversion_and_inference.ipynb
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mnist_model.onnx
â”‚   â”œâ”€â”€ mnist_model_quant_uint8.onnx
â”‚   â”œâ”€â”€ mnist_model.tflite
â”‚   â””â”€â”€ mnist_model_fp16.tflite
â”œâ”€â”€ README.md


---

## ðŸš€ Next Steps

- Try converting to TFLite and run on microcontrollers
- Test ONNX inference on Raspberry Pi or ARM boards
- Explore static quantization or QAT

---
## ðŸ“„ Model Card (Mini)

**Model**: Small CNN for MNIST digit classification (2 conv + 2 FC)  
**Task**: Image classification (10 classes, 28Ã—28 grayscale)  
**Datasets**: MNIST (train/test splits from Keras & TorchVision)  
**Training**: Adam, 1â€“2 epochs (demo scale), crossâ€‘entropy loss  
**Intended use**: Educational demo for embeddedâ€‘AI workflows (PyTorch â†’ ONNX â†’ TFLite), not for production  
**Metrics (indicative)**: 97â€“99% test accuracy after 2 epochs (varies), TFLite latency ~0.025 ms (CPU sim)  
**Artifacts**:
- `models/mnist_model.onnx` (original ONNX)
- `models/mnist_model_quant_uint8.onnx` (ONNX dynamic quantized)
- `models/mnist_model.tflite` (optimized float)
- `models/mnist_model_fp16.tflite` (FP16 weights)
**Limitations**:
- Trained briefly; not robust to domain shift or adversarial inputs
- CPU benchmarks simulate embedded behavior; real devices may differ
- ONNX Runtime CPU does not accelerate ConvInteger ops; quantized speedups are hardwareâ€‘dependent
**License**: MIT (see LICENSE)


## ðŸ§  Author

**Shravan Suryanarayana**  
System Software Architect | Embedded AI Explorer  
[LinkedIn](https://linkedin.com/in/shravansurya)
