# embedded-ai-model-export
Hands-on project: PyTorch â†’ ONNX â†’ Quantization â†’ Inference (MNIST example)

# PyTorch â†’ ONNX â†’ Quantization Demo (MNIST)

This is a hands-on mini-project to demonstrate:
- Training a simple CNN using PyTorch on the MNIST dataset
- Exporting the model to ONNX format
- Quantizing the ONNX model to reduce its size
- Running inference using ONNX Runtime
- Benchmarking performance of original vs quantized models

---

## âœ… What I Learned

- How to convert a PyTorch model to ONNX
- How to apply dynamic quantization with ONNX Runtime
- How quantization affects model size and inference time
- Practical insights into edge/embedded AI deployment pipelines

---

## ğŸ§ª Results

### Prediction:
Expected inference for: (digit 4)
![Digit Prediction](./digit_prediction_example.png)

Outcome:
output: ONNX Prediction: 4 | Actual Label: 4

### Execution time comparison:

| Model                 | Size     | Inference Time (CPU) |
|----------------------|----------|-----------------------|
| `mnist_model.onnx`   | ~1.3 MB  | ~0.023 ms             |
| `mnist_model_quant_uint8.onnx` | ~450 KB | ~0.043 ms  |

> âš ï¸ ONNX Runtime on x86 doesnâ€™t accelerate ConvInteger ops. Better gains expected on ARM/embedded targets.

---

## ğŸ“‚ Project Structure

embedded-ai-model-export/
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ mnist_model.onnx
â”‚ â””â”€â”€ mnist_model_quant_uint8.onnx
â”œâ”€â”€ pytorch_onnx_quantization_demo.ipynb
â”œâ”€â”€ README.md

---

## ğŸš€ Next Steps

- Try converting to TFLite and run on microcontrollers
- Test ONNX inference on Raspberry Pi or ARM boards
- Explore static quantization or QAT

---

## ğŸ§  Author

**Shravan Suryanarayana**  
System Software Architect | Embedded AI Explorer  
[LinkedIn](https://linkedin.com/in/shravansurya)
